{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.create_experiment(\"Boston-HousePrice-predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/06/26 11:17:08 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'eed009ca094144269acde54e91554aaa', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2024/06/26 11:17:08 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n",
      "2024/06/26 11:17:10 INFO mlflow.sklearn.utils: Logging the 5 best runs, no runs will be omitted.\n",
      "2024/06/26 11:17:11 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '53b1f92ac80a475091573c335a9df54d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2024/06/26 11:17:11 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n",
      "2024/06/26 11:17:13 INFO mlflow.sklearn.utils: Logging the 5 best runs, no runs will be omitted.\n",
      "2024/06/26 11:17:14 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'bbbad8fa675545cdb67ad3b42aead4fa', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2024/06/26 11:17:14 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n",
      "2024/06/26 11:17:17 INFO mlflow.sklearn.utils: Logging the 5 best runs, 4 runs will be omitted.\n",
      "2024/06/26 11:17:18 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'bbfc0fa845e04221970f38a0f763ab29', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2024/06/26 11:17:18 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n",
      "2024/06/26 11:17:33 INFO mlflow.sklearn.utils: Logging the 5 best runs, 4 runs will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best parameter: {'Ridge': Ridge(alpha=10.0), 'Lasso': Lasso(alpha=0.1), 'SVR': SVR(C=10.0), 'RandomForest': RandomForestRegressor(max_depth=10, n_estimators=50)}\n",
      "Ridge: MSE = 24.495845619667406, R2 = 0.6659677905050341\n",
      "Lasso: MSE = 25.656739367167685, R2 = 0.6501375183238985\n",
      "SVR: MSE = 12.738202847389026, R2 = 0.8262982993862371\n",
      "RandomForest: MSE = 9.043600082330494, R2 = 0.8766789371474359\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def get_model(model_path):\n",
    "    module_name, class_name = model_path.rsplit('.', 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    model_class = getattr(module, class_name)\n",
    "    return model_class()\n",
    "\n",
    "def train_models(X, y, config):\n",
    "    models = {name: get_model(path) for name, path in config['models'].items()}\n",
    "    params = config['params']\n",
    "    best_estimators = {}\n",
    "    for name, model in models.items():\n",
    "        grid = GridSearchCV(model, params[name], cv=10, scoring='neg_mean_squared_error')\n",
    "        grid.fit(X, y)\n",
    "        best_estimators[name] = grid.best_estimator_\n",
    "        # Log the best parameters\n",
    "        with mlflow.start_run(run_name=f\"{name}_grid_best_estimator\"):\n",
    "            mlflow.log_params(grid.best_params_)\n",
    "            mlflow.sklearn.log_model(grid.best_estimator_, artifact_path=f\"{name}_model\")\n",
    "            y_pred_train = grid.best_estimator_.predict(X)\n",
    "            mse = mean_squared_error(y, y_pred_train)\n",
    "            r2 = r2_score(y, y_pred_train)\n",
    "            mlflow.log_metric('MSE', mse)\n",
    "            mlflow.log_metric('R2', r2)\n",
    "    return best_estimators\n",
    "\n",
    "def evaluate_models(models, X_test, y_test):\n",
    "    results = {}\n",
    "    mlflow.set_experiment(\"Boston-HousePrice-predict\")\n",
    "    for name, model in models.items():\n",
    "        with mlflow.start_run(run_name=f\"{name}_models\",experiment_id=\"593623705617405727\", nested=True):\n",
    "            y_pred = model.predict(X_test)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            results[name] = {'MSE': mse, 'R2': r2}\n",
    "            mlflow.log_metric('MSE', mse)\n",
    "            mlflow.log_metric('R2', r2)\n",
    "            mlflow.sklearn.log_model(model, artifact_path=f\"{name}_model\")\n",
    "            # Log model parameters\n",
    "            mlflow.log_params(model.get_params())\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Load the config file\n",
    "    mlflow.sklearn.autolog()\n",
    "    config = load_config('./config.json')\n",
    "    data = pd.read_csv('Boston Housing.csv')\n",
    "    X = data.drop('medv', axis=1)\n",
    "    y = data['medv']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    models = train_models(X_train_scaled, y_train, config)\n",
    "    print(\"the best parameter:\", models)\n",
    "    results = evaluate_models(models, X_test_scaled, y_test)\n",
    "    for model, metrics in results.items():\n",
    "        if 'MSE' in metrics:\n",
    "            print(f\"{model}: MSE = {metrics['MSE']}, R2 = {metrics['R2']}\")\n",
    "        else:\n",
    "            print(f\"{model}: Accuracy = {metrics['Accuracy']}, F1 = {metrics['F1']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
